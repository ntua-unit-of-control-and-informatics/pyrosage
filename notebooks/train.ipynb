{
 "cells": [
  {
   "cell_type": "code",
   "id": "7ab3b66f4aa6c8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T08:42:07.197528Z",
     "start_time": "2025-05-26T08:41:39.463076Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "import pandas as pd\n",
    "from AttentiveFP import get_smiles_dicts, get_smiles_array, num_atom_features, num_bond_features  # assumed to be your actual featurizer\n",
    "from AttentiveFP import Fingerprint  # assumed to be your attentive FP model"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "06510a14-aa01-4d43-a271-8038c4a35bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T08:42:07.568049Z",
     "start_time": "2025-05-26T08:42:07.564798Z"
    }
   },
   "source": [
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'AMES'\n",
    "\n",
    "# --- Dataset ---\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, smiles_list, targets, feature_dicts):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.targets = targets\n",
    "        self.feature_dicts = feature_dicts\n",
    "        self.x_atom, self.x_bond, self.x_atom_index, self.x_bond_index, self.x_mask, _ = get_smiles_array(smiles_list, feature_dicts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.x_atom[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_bond[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_atom_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_bond_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_mask[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a097b977-66fa-42b5-9a83-640e635c0a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T08:45:50.957091Z",
     "start_time": "2025-05-26T08:44:01.461380Z"
    }
   },
   "source": [
    "# --- Load data ---\n",
    "df = pd.read_csv(f\"../data/{model_name}.csv\")\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "targets = df[\"active\"].tolist()\n",
    "feature_dicts = get_smiles_dicts(smiles_list)\n",
    "\n",
    "# --- Dataloader ---\n",
    "dataset = MoleculeDataset(smiles_list, targets, feature_dicts)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'CC(C)[C@@H]1CC[C@H](C)[C@@H]2CC[C@H](C)C[C@@H]12'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyError\u001B[39m                                  Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[4]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m feature_dicts = get_smiles_dicts(smiles_list)\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# --- Dataloader ---\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m dataset = \u001B[43mMoleculeDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msmiles_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_dicts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m loader = DataLoader(dataset, batch_size=\u001B[32m32\u001B[39m, shuffle=\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mMoleculeDataset.__init__\u001B[39m\u001B[34m(self, smiles_list, targets, feature_dicts)\u001B[39m\n\u001B[32m      9\u001B[39m \u001B[38;5;28mself\u001B[39m.targets = targets\n\u001B[32m     10\u001B[39m \u001B[38;5;28mself\u001B[39m.feature_dicts = feature_dicts\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m \u001B[38;5;28mself\u001B[39m.x_atom, \u001B[38;5;28mself\u001B[39m.x_bond, \u001B[38;5;28mself\u001B[39m.x_atom_index, \u001B[38;5;28mself\u001B[39m.x_bond_index, \u001B[38;5;28mself\u001B[39m.x_mask, _ = \u001B[43mget_smiles_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43msmiles_list\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_dicts\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/ntua/pyrosage/notebooks/AttentiveFP/getFeatures.py:418\u001B[39m, in \u001B[36mget_smiles_array\u001B[39m\u001B[34m(smilesList, feature_dicts)\u001B[39m\n\u001B[32m    416\u001B[39m x_bond_index = []\n\u001B[32m    417\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m smiles \u001B[38;5;129;01min\u001B[39;00m smilesList:\n\u001B[32m--> \u001B[39m\u001B[32m418\u001B[39m     x_mask.append(\u001B[43mfeature_dicts\u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m'\u001B[39;49m\u001B[33;43msmiles_to_atom_mask\u001B[39;49m\u001B[33;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[43msmiles\u001B[49m\u001B[43m]\u001B[49m)\n\u001B[32m    419\u001B[39m     x_atom.append(feature_dicts[\u001B[33m'\u001B[39m\u001B[33msmiles_to_atom_info\u001B[39m\u001B[33m'\u001B[39m][smiles])\n\u001B[32m    420\u001B[39m     x_bonds.append(feature_dicts[\u001B[33m'\u001B[39m\u001B[33msmiles_to_bond_info\u001B[39m\u001B[33m'\u001B[39m][smiles])\n",
      "\u001B[31mKeyError\u001B[39m: 'CC(C)[C@@H]1CC[C@H](C)[C@@H]2CC[C@H](C)C[C@@H]12'"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "723ac54d-507c-4fd1-8035-3f137871e54d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T08:42:07.808551Z",
     "start_time": "2025-05-22T11:39:10.684038Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Model ---\n",
    "model = Fingerprint(\n",
    "    radius=5,\n",
    "    T=3,\n",
    "    input_feature_dim=num_atom_features(),\n",
    "    input_bond_dim=num_bond_features(),\n",
    "    fingerprint_dim=200,\n",
    "    output_units_num=1,\n",
    "    p_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408a5e5c-7050-4706-b316-17942f475f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split SMILES and targets\n",
    "train_smiles, val_smiles, train_targets, val_targets = train_test_split(\n",
    "    smiles_list, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_set = MoleculeDataset(train_smiles, train_targets, feature_dicts)\n",
    "val_set = MoleculeDataset(val_smiles, val_targets, feature_dicts)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164a302e-4f5f-4725-b528-9b2ece383ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T08:42:07.810308Z",
     "start_time": "2025-05-22T11:39:10.684038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 163.0026 - Val Loss: 29.7152\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 29.7152)\n",
      "Epoch 2 - Train Loss: 118.6357 - Val Loss: 25.5445\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 25.5445)\n",
      "Epoch 3 - Train Loss: 114.7395 - Val Loss: 28.9259\n",
      "Epoch 4 - Train Loss: 108.3598 - Val Loss: 23.9633\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 23.9633)\n",
      "Epoch 5 - Train Loss: 104.8730 - Val Loss: 25.9855\n",
      "Epoch 6 - Train Loss: 104.4509 - Val Loss: 24.6480\n",
      "Epoch 7 - Train Loss: 99.7017 - Val Loss: 22.6848\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 22.6848)\n",
      "Epoch 8 - Train Loss: 96.1251 - Val Loss: 22.0022\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 22.0022)\n",
      "Epoch 9 - Train Loss: 93.6719 - Val Loss: 21.7462\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 21.7462)\n",
      "Epoch 10 - Train Loss: 89.9997 - Val Loss: 20.2040\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 20.2040)\n",
      "Epoch 11 - Train Loss: 86.0181 - Val Loss: 21.6333\n",
      "Epoch 12 - Train Loss: 84.7447 - Val Loss: 19.6141\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 19.6141)\n",
      "Epoch 13 - Train Loss: 81.1431 - Val Loss: 19.6184\n",
      "Epoch 14 - Train Loss: 77.7889 - Val Loss: 18.3320\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 18.3320)\n",
      "Epoch 15 - Train Loss: 75.2583 - Val Loss: 18.1750\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 18.1750)\n",
      "Epoch 16 - Train Loss: 75.3686 - Val Loss: 18.7656\n",
      "Epoch 17 - Train Loss: 70.4634 - Val Loss: 18.3905\n",
      "Epoch 18 - Train Loss: 68.1594 - Val Loss: 17.9476\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 17.9476)\n",
      "Epoch 19 - Train Loss: 63.3750 - Val Loss: 18.2529\n",
      "Epoch 20 - Train Loss: 62.3719 - Val Loss: 18.0264\n",
      "Epoch 21 - Train Loss: 58.4332 - Val Loss: 17.3947\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 17.3947)\n",
      "Epoch 22 - Train Loss: 56.2217 - Val Loss: 17.2183\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 17.2183)\n",
      "Epoch 23 - Train Loss: 55.9990 - Val Loss: 16.0693\n",
      "✅ Saved best model to ../models/model_Z50.pt (val loss: 16.0693)\n",
      "Epoch 24 - Train Loss: 51.5693 - Val Loss: 16.9394\n",
      "Epoch 25 - Train Loss: 49.5740 - Val Loss: 17.0459\n",
      "Epoch 26 - Train Loss: 48.2349 - Val Loss: 16.2392\n",
      "Epoch 27 - Train Loss: 45.9964 - Val Loss: 16.1510\n",
      "Epoch 28 - Train Loss: 43.2855 - Val Loss: 16.3548\n",
      "Epoch 29 - Train Loss: 40.6999 - Val Loss: 16.4890\n",
      "Epoch 30 - Train Loss: 39.6946 - Val Loss: 16.8107\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Folder to save model\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "# Load existing model\n",
    "# model.load_state_dict(torch.load(\"../models/model_AMES.pt\"))\n",
    "# model.load_state_dict(torch.load(f\"../models/model_{model_name}.pt\"))\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for atom, bond, atom_deg, bond_deg, mask, target in train_loader:\n",
    "        atom, bond, atom_deg, bond_deg, mask, target = [\n",
    "            t.to(device) for t in (atom, bond, atom_deg, bond_deg, mask, target)\n",
    "        ]\n",
    "        _, pred, _ = model(atom, bond, atom_deg, bond_deg, mask)\n",
    "        loss = loss_fn(pred.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for atom, bond, atom_deg, bond_deg, mask, target in val_loader:\n",
    "            atom, bond, atom_deg, bond_deg, mask, target = [\n",
    "                t.to(device) for t in (atom, bond, atom_deg, bond_deg, mask, target)\n",
    "            ]\n",
    "            _, pred, _ = model(atom, bond, atom_deg, bond_deg, mask)\n",
    "            loss = loss_fn(pred.squeeze(), target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        path = \"../models/model_AMES.pt\"\n",
    "        path = f\"../models/model_{model_name}.pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"✅ Saved best model to {path} (val loss: {best_loss:.4f})\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrosage",
   "language": "python",
   "name": "pyrosage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
