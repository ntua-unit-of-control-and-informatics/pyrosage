{
 "cells": [
  {
   "cell_type": "code",
   "id": "7ab3b66f4aa6c8f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:06:45.832986Z",
     "start_time": "2025-05-26T09:06:40.205747Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "import pandas as pd\n",
    "from AttentiveFP import get_smiles_dicts, get_smiles_array, num_atom_features, num_bond_features  # assumed to be your actual featurizer\n",
    "from AttentiveFP import Fingerprint  # assumed to be your attentive FP model"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "06510a14-aa01-4d43-a271-8038c4a35bef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:06:46.124238Z",
     "start_time": "2025-05-26T09:06:46.121020Z"
    }
   },
   "source": [
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = 'Endocrine_Disruption_NR-AhR'\n",
    "\n",
    "# --- Dataset ---\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, smiles_list, targets, feature_dicts):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.targets = targets\n",
    "        self.feature_dicts = feature_dicts\n",
    "        self.x_atom, self.x_bond, self.x_atom_index, self.x_bond_index, self.x_mask, _ = get_smiles_array(smiles_list, feature_dicts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feature_dicts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.x_atom[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_bond[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_atom_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_bond_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_mask[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "a097b977-66fa-42b5-9a83-640e635c0a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:08:41.859837Z",
     "start_time": "2025-05-26T09:06:46.146995Z"
    }
   },
   "source": [
    "# --- Load data ---\n",
    "df = pd.read_csv(f\"../data/{model_name}.csv\")\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "targets = df[\"active\"].tolist()\n",
    "feature_dicts = get_smiles_dicts(smiles_list)\n",
    "\n",
    "# --- Dataloader ---\n",
    "dataset = MoleculeDataset(smiles_list, targets, feature_dicts)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: SMILES CC(C)[C@@H]1CC[C@H](C)[C@@H]2CC[C@H](C)C[C@@H]12 not found in feature dictionaries\n",
      "Warning: SMILES ClC1=C(Cl)[C@]2(Cl)[C@@H]3[C@@H](Cl)C=C[C@H]3[C@@]1(Cl)C2(Cl)Cl not found in feature dictionaries\n",
      "Warning: SMILES C[C@H]1CN(N=O)C[C@@H](C)N1N=O not found in feature dictionaries\n",
      "Warning: SMILES N=c1ccn2c(n1)O[C@@H]1[C@@H]2O[C@H](CO)[C@H]1O not found in feature dictionaries\n",
      "Warning: SMILES OC[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO not found in feature dictionaries\n",
      "Warning: SMILES C1O[C@H]1[C@@H]1CC[C@@H]2O[C@@H]2C1 not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC(C=O)=C(O)C[C@@H]1CC[C@@H]1[C@@H]2CC[C@@]2(C)[C@@H]1CC[C@]2(C)O not found in feature dictionaries\n",
      "Warning: SMILES CC[C@H](CC[C@@H](C)[C@H]1CC[C@H]2[C@@H]3CC=C4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@@]21C)C(C)C not found in feature dictionaries\n",
      "Warning: SMILES O=C(O)[C@]1(O)C[C@H](O)[C@@H](O)[C@H](O)C1 not found in feature dictionaries\n",
      "Warning: SMILES CO[C@]12[C@H]3N[C@H]3CN1C1=C(C(=O)C(N)=C(C)C1=O)[C@@H]2COC(N)=O not found in feature dictionaries\n",
      "Warning: SMILES Nc1ccn([C@@H]2O[C@H](COP(=O)(O)O[C@@H]3[C@@H](CO)O[C@@H](n4cnc5c(N)ncnc54)[C@@H]3O)[C@@H](O)[C@H]2O)c(=O)n1 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H](CBr)[C@H](O)[C@H](O)[C@@H](O)CBr not found in feature dictionaries\n",
      "Warning: SMILES c1cc2c3c(cccc3c1)[C@H]1N[C@@H]21 not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12C[C@H](O)[C@H]3[C@@H](CCC4=CC(=O)C=C[C@@]43C)[C@@H]1CC[C@]2(O)C(=O)CO not found in feature dictionaries\n",
      "Warning: SMILES C[C@@H]1OCCc2cc3c(cc21)C(C)(C)[C@@H](C)C3(C)C not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1[C@H](O)c2cc3ccc4ccccc4c3cc2[C@@H]2O[C@@H]21 not found in feature dictionaries\n",
      "Warning: SMILES OC[C@H](O)[C@@H](O)[C@H](O[C@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O)[C@H](O)CO not found in feature dictionaries\n",
      "Warning: SMILES CN1[C@H]2C[C@@H](OC(=O)[C@@H](CO)c3ccccc3)C[C@@H]1[C@@H]1O[C@@H]12 not found in feature dictionaries\n",
      "Warning: SMILES C[C@@H](CCC(=O)O)[C@H]1CC[C@H]2[C@@H]3CC[C@H]4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@@]21C not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC[C@H]4C[C@@H](OC(=O)COc5ccc(N(CCCl)CCCl)cc5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES Br[C@H]1CC[C@@H](Br)[C@H](Br)CC[C@H](Br)[C@H](Br)CC[C@H]1Br not found in feature dictionaries\n",
      "Warning: SMILES C[C@H]1CN(N=O)[C@H](C)CN1N=O not found in feature dictionaries\n",
      "Warning: SMILES CC(C)CCC[C@@H](C)[C@@H]1CC[C@@H]2[C@@H]3CC4([C@@H]5C[C@@H](Br)CC[C@]5(C)[C@@H]3CC[C@]12C)S(=O)(=O)CCS4(=O)=O not found in feature dictionaries\n",
      "Warning: SMILES ClC1=C(Cl)[C@]2(Cl)[C@H]3[C@H]4C=C[C@H](C4)[C@H]3[C@@]1(Cl)C2(Cl)Cl not found in feature dictionaries\n",
      "Warning: SMILES C[C@@]12CCC3[C@]4(C)CCC[C@@](C)(C(=O)O)C4CC[C@]3(CC1=O)C2 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1c2c(cc3ccc4cccc5ccc2c3c45)[C@@H]2O[C@@H]2[C@H]1O not found in feature dictionaries\n",
      "Warning: SMILES C1=C[C@@H]2[C@H]3C=C[C@H](C3)[C@@H]2C1 not found in feature dictionaries\n",
      "Warning: SMILES CCOC(=O)[C@@H](C)C[C@](C)(C[C@H](C[C@@](C)(C[C@@H](C[C@](C)(CC)C(=O)O)C(=O)OCC)C(=O)O)C(=O)OCC)C(=O)O not found in feature dictionaries\n",
      "Warning: SMILES CCCCCCCCCCCCCC(=O)O[C@@H]1[C@H](C)[C@]2(O)[C@@H](C=C(CO)C[C@@]3(O)C(=O)C(C)=C[C@H]32)[C@H]2C(C)(C)[C@]21OC(C)=O not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC=C4C[C@@H](OC(=O)/C=C/c5cccc(N(CCCl)CCCl)c5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES COC(=O)C[C@@H]1[C@@]2(C)C(=O)C=C[C@@]3(C)C(=O)O[C@H](C32)[C@H]2OC3C[C@@H](c4ccoc4)C(C)=C3[C@]21C not found in feature dictionaries\n",
      "Warning: SMILES C1C[C@@H]2O[C@@H]2[C@H]1O[C@H]1CC[C@@H]2O[C@H]12 not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC[C@H]4C[C@@H](OC(=O)Cc5ccc(N(CCCl)CCCl)cc5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1c2cccc3c2[C@@]2(O[C@@H]2[C@@H]1O)c1ccc2ccccc2c1-3 not found in feature dictionaries\n",
      "Warning: SMILES ClC1(Cl)[C@]2(Cl)[C@@]3(Cl)[C@]4(Cl)C(Cl)(Cl)[C@]5(Cl)[C@@]3(Cl)[C@@]1(Cl)[C@]5(Cl)[C@@]42Cl not found in feature dictionaries\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "723ac54d-507c-4fd1-8035-3f137871e54d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:08:46.467599Z",
     "start_time": "2025-05-26T09:08:41.895486Z"
    }
   },
   "source": [
    "# --- Model ---\n",
    "model = Fingerprint(\n",
    "    radius=5,\n",
    "    T=3,\n",
    "    input_feature_dim=num_atom_features(),\n",
    "    input_bond_dim=num_bond_features(),\n",
    "    fingerprint_dim=200,\n",
    "    output_units_num=1,\n",
    "    p_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = MSELoss()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "408a5e5c-7050-4706-b316-17942f475f1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:08:49.131757Z",
     "start_time": "2025-05-26T09:08:46.479854Z"
    }
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split SMILES and targets\n",
    "train_smiles, val_smiles, train_targets, val_targets = train_test_split(\n",
    "    smiles_list, targets, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_set = MoleculeDataset(train_smiles, train_targets, feature_dicts)\n",
    "val_set = MoleculeDataset(val_smiles, val_targets, feature_dicts)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32, shuffle=False)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: SMILES C1C[C@@H]2O[C@@H]2[C@H]1O[C@H]1CC[C@@H]2O[C@H]12 not found in feature dictionaries\n",
      "Warning: SMILES C[C@H]1CN(N=O)[C@H](C)CN1N=O not found in feature dictionaries\n",
      "Warning: SMILES CC(C)[C@@H]1CC[C@H](C)[C@@H]2CC[C@H](C)C[C@@H]12 not found in feature dictionaries\n",
      "Warning: SMILES C[C@@H](CCC(=O)O)[C@H]1CC[C@H]2[C@@H]3CC[C@H]4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@@]21C not found in feature dictionaries\n",
      "Warning: SMILES C1=C[C@@H]2[C@H]3C=C[C@H](C3)[C@@H]2C1 not found in feature dictionaries\n",
      "Warning: SMILES C[C@H]1CN(N=O)C[C@@H](C)N1N=O not found in feature dictionaries\n",
      "Warning: SMILES c1cc2c3c(cccc3c1)[C@H]1N[C@@H]21 not found in feature dictionaries\n",
      "Warning: SMILES OC[C@H](O)[C@@H](O)[C@H](O[C@H]1O[C@H](CO)[C@@H](O)[C@H](O)[C@H]1O)[C@H](O)CO not found in feature dictionaries\n",
      "Warning: SMILES Nc1ccn([C@@H]2O[C@H](COP(=O)(O)O[C@@H]3[C@@H](CO)O[C@@H](n4cnc5c(N)ncnc54)[C@@H]3O)[C@@H](O)[C@H]2O)c(=O)n1 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1c2c(cc3ccc4cccc5ccc2c3c45)[C@@H]2O[C@@H]2[C@H]1O not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC(C=O)=C(O)C[C@@H]1CC[C@@H]1[C@@H]2CC[C@@]2(C)[C@@H]1CC[C@]2(C)O not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1c2cccc3c2[C@@]2(O[C@@H]2[C@@H]1O)c1ccc2ccccc2c1-3 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H](CBr)[C@H](O)[C@H](O)[C@@H](O)CBr not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12C[C@H](O)[C@H]3[C@@H](CCC4=CC(=O)C=C[C@@]43C)[C@@H]1CC[C@]2(O)C(=O)CO not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC[C@H]4C[C@@H](OC(=O)COc5ccc(N(CCCl)CCCl)cc5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES O[C@H]1[C@H](O)c2cc3ccc4ccccc4c3cc2[C@@H]2O[C@@H]21 not found in feature dictionaries\n",
      "Warning: SMILES ClC1=C(Cl)[C@]2(Cl)[C@@H]3[C@@H](Cl)C=C[C@H]3[C@@]1(Cl)C2(Cl)Cl not found in feature dictionaries\n",
      "Warning: SMILES CCOC(=O)[C@@H](C)C[C@](C)(C[C@H](C[C@@](C)(C[C@@H](C[C@](C)(CC)C(=O)O)C(=O)OCC)C(=O)O)C(=O)OCC)C(=O)O not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC=C4C[C@@H](OC(=O)/C=C/c5cccc(N(CCCl)CCCl)c5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES COC(=O)C[C@@H]1[C@@]2(C)C(=O)C=C[C@@]3(C)C(=O)O[C@H](C32)[C@H]2OC3C[C@@H](c4ccoc4)C(C)=C3[C@]21C not found in feature dictionaries\n",
      "Warning: SMILES CO[C@]12[C@H]3N[C@H]3CN1C1=C(C(=O)C(N)=C(C)C1=O)[C@@H]2COC(N)=O not found in feature dictionaries\n",
      "Warning: SMILES C[C@@]12CCC3[C@]4(C)CCC[C@@](C)(C(=O)O)C4CC[C@]3(CC1=O)C2 not found in feature dictionaries\n",
      "Warning: SMILES ClC1(Cl)[C@]2(Cl)[C@@]3(Cl)[C@]4(Cl)C(Cl)(Cl)[C@]5(Cl)[C@@]3(Cl)[C@@]1(Cl)[C@]5(Cl)[C@@]42Cl not found in feature dictionaries\n",
      "Warning: SMILES CC[C@H](CC[C@@H](C)[C@H]1CC[C@H]2[C@@H]3CC=C4C[C@@H](O)CC[C@]4(C)[C@H]3CC[C@@]21C)C(C)C not found in feature dictionaries\n",
      "Warning: SMILES CCCCCCCCCCCCCC(=O)O[C@@H]1[C@H](C)[C@]2(O)[C@@H](C=C(CO)C[C@@]3(O)C(=O)C(C)=C[C@H]32)[C@H]2C(C)(C)[C@]21OC(C)=O not found in feature dictionaries\n",
      "Warning: SMILES CC(C)CCC[C@@H](C)[C@@H]1CC[C@@H]2[C@@H]3CC4([C@@H]5C[C@@H](Br)CC[C@]5(C)[C@@H]3CC[C@]12C)S(=O)(=O)CCS4(=O)=O not found in feature dictionaries\n",
      "Warning: SMILES Br[C@H]1CC[C@@H](Br)[C@H](Br)CC[C@H](Br)[C@H](Br)CC[C@H]1Br not found in feature dictionaries\n",
      "Warning: SMILES CN1[C@H]2C[C@@H](OC(=O)[C@@H](CO)c3ccccc3)C[C@@H]1[C@@H]1O[C@@H]12 not found in feature dictionaries\n",
      "Warning: SMILES C[C@@H]1OCCc2cc3c(cc21)C(C)(C)[C@@H](C)C3(C)C not found in feature dictionaries\n",
      "Warning: SMILES C[C@]12CC[C@H]3[C@@H](CC[C@H]4C[C@@H](OC(=O)Cc5ccc(N(CCCl)CCCl)cc5)CC[C@@]43C)[C@@H]1CCC(=O)N2 not found in feature dictionaries\n",
      "Warning: SMILES OC[C@H](O)[C@@H](O)[C@H](O)[C@H](O)CO not found in feature dictionaries\n",
      "Warning: SMILES O=C(O)[C@]1(O)C[C@H](O)[C@@H](O)[C@H](O)C1 not found in feature dictionaries\n",
      "Warning: SMILES N=c1ccn2c(n1)O[C@@H]1[C@@H]2O[C@H](CO)[C@H]1O not found in feature dictionaries\n",
      "Warning: SMILES ClC1=C(Cl)[C@]2(Cl)[C@H]3[C@H]4C=C[C@H](C4)[C@H]3[C@@]1(Cl)C2(Cl)Cl not found in feature dictionaries\n",
      "Warning: SMILES C1O[C@H]1[C@@H]1CC[C@@H]2O[C@@H]2C1 not found in feature dictionaries\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "164a302e-4f5f-4725-b528-9b2ece383ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T09:08:49.381983Z",
     "start_time": "2025-05-26T09:08:49.145665Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Folder to save model\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "try:\n",
    "    # Load existing model\n",
    "    model.load_state_dict(torch.load(f\"../models/model_{model_name}.pt\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "except:\n",
    "    print(\"The model could not be loaded. Training a new model from scratch.\")\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for atom, bond, atom_deg, bond_deg, mask, target in train_loader:\n",
    "        atom, bond, atom_deg, bond_deg, mask, target = [\n",
    "            t.to(device) for t in (atom, bond, atom_deg, bond_deg, mask, target)\n",
    "        ]\n",
    "        _, pred, _ = model(atom, bond, atom_deg, bond_deg, mask)\n",
    "        loss = loss_fn(pred.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for atom, bond, atom_deg, bond_deg, mask, target in val_loader:\n",
    "            atom, bond, atom_deg, bond_deg, mask, target = [\n",
    "                t.to(device) for t in (atom, bond, atom_deg, bond_deg, mask, target)\n",
    "            ]\n",
    "            _, pred, _ = model(atom, bond, atom_deg, bond_deg, mask)\n",
    "            loss = loss_fn(pred.squeeze(), target)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        path = f\"../models/model_{model_name}.pt\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"âœ… Saved best model to {path} (val loss: {best_loss:.4f})\")\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library.",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 24\u001B[39m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m atom, bond, atom_deg, bond_deg, mask, target \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[32m     21\u001B[39m     atom, bond, atom_deg, bond_deg, mask, target = [\n\u001B[32m     22\u001B[39m         t.to(device) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m (atom, bond, atom_deg, bond_deg, mask, target)\n\u001B[32m     23\u001B[39m     ]\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m     _, pred, _ = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43matom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matom_deg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbond_deg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     25\u001B[39m     loss = loss_fn(pred.squeeze(), target)\n\u001B[32m     27\u001B[39m     optimizer.zero_grad()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/ntua/pyrosage/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/ntua/pyrosage/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Projects/ntua/pyrosage/notebooks/AttentiveFP/AttentiveLayers.py:50\u001B[39m, in \u001B[36mFingerprint.forward\u001B[39m\u001B[34m(self, atom_list, bond_list, atom_degree_list, bond_degree_list, atom_mask)\u001B[39m\n\u001B[32m     48\u001B[39m attend_mask[attend_mask != mol_length-\u001B[32m1\u001B[39m] = \u001B[32m1\u001B[39m\n\u001B[32m     49\u001B[39m attend_mask[attend_mask == mol_length-\u001B[32m1\u001B[39m] = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m attend_mask = \u001B[43mattend_mask\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcuda\u001B[49m\u001B[43m.\u001B[49m\u001B[43mFloatTensor\u001B[49m\u001B[43m)\u001B[49m.unsqueeze(-\u001B[32m1\u001B[39m)\n\u001B[32m     52\u001B[39m softmax_mask = atom_degree_list.clone()\n\u001B[32m     53\u001B[39m softmax_mask[softmax_mask != mol_length-\u001B[32m1\u001B[39m] = \u001B[32m0\u001B[39m\n",
      "\u001B[31mRuntimeError\u001B[39m: Cannot initialize CUDA without ATen_cuda library. PyTorch splits its backend into two shared libraries: a CPU library and a CUDA library; this error has occurred because you are trying to use some CUDA functionality, but the CUDA library has not been loaded by the dynamic linker for some reason.  The CUDA library MUST be loaded, EVEN IF you don't directly use any symbols from the CUDA library! One common culprit is a lack of -Wl,--no-as-needed in your link arguments; many dynamic linkers will delete dynamic library dependencies if you don't depend on any of their symbols.  You can check if this has occurred by using ldd on your binary to see if there is a dependency on *_cuda.so library."
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyrosage",
   "language": "python",
   "name": "pyrosage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
