{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:39:09.617629Z",
     "start_time": "2025-05-22T11:39:08.084512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "import pandas as pd\n",
    "from AttentiveFP import get_smiles_dicts, get_smiles_array, num_atom_features, num_bond_features  # assumed to be your actual featurizer\n",
    "from AttentiveFP import Fingerprint  # assumed to be your attentive FP model"
   ],
   "id": "7ab3b66f4aa6c8f0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-22T11:39:17.947692Z",
     "start_time": "2025-05-22T11:39:10.684038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Dataset ---\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, smiles_list, targets, feature_dicts):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.targets = targets\n",
    "        self.feature_dicts = feature_dicts\n",
    "        self.x_atom, self.x_bond, self.x_atom_index, self.x_bond_index, self.x_mask, _ = get_smiles_array(smiles_list, feature_dicts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.x_atom[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_bond[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.x_atom_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_bond_index[idx], dtype=torch.long),\n",
    "            torch.tensor(self.x_mask[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "# --- Load data ---\n",
    "df = pd.read_csv(\"../data/LD50_Zhu.csv\")  # your dataset path\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "targets = df[\"active\"].tolist()\n",
    "feature_dicts = get_smiles_dicts(smiles_list)\n",
    "\n",
    "# --- Dataloader ---\n",
    "dataset = MoleculeDataset(smiles_list, targets, feature_dicts)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# --- Model ---\n",
    "model = Fingerprint(\n",
    "    radius=5,\n",
    "    T=3,\n",
    "    input_feature_dim=num_atom_features(),\n",
    "    input_bond_dim=num_bond_features(),\n",
    "    fingerprint_dim=200,\n",
    "    output_units_num=1,\n",
    "    p_dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = MSELoss()\n",
    "\n",
    "# --- Training loop ---\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for atom, bond, atom_deg, bond_deg, mask, target in loader:\n",
    "        atom = atom.to(device)\n",
    "        bond = bond.to(device)\n",
    "        atom_deg = atom_deg.to(device)\n",
    "        bond_deg = bond_deg.to(device)\n",
    "        mask = mask.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        _, pred, _ = model(atom, bond, atom_deg, bond_deg, mask)\n",
    "        loss = loss_fn(pred.squeeze(), target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss:.4f}\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 52\u001B[0m\n\u001B[1;32m     49\u001B[0m total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m atom, bond, atom_deg, bond_deg, mask, target \u001B[38;5;129;01min\u001B[39;00m loader:\n\u001B[0;32m---> 52\u001B[0m     _, pred, _ \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43matom\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbond\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43matom_deg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbond_deg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(pred\u001B[38;5;241m.\u001B[39msqueeze(), target)\n\u001B[1;32m     55\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[0;32m~/Projects/ntua/pyrosage/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/ntua/pyrosage/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Projects/ntua/pyrosage/notebooks/AttentiveFP/AttentiveLayers.py:50\u001B[0m, in \u001B[0;36mFingerprint.forward\u001B[0;34m(self, atom_list, bond_list, atom_degree_list, bond_degree_list, atom_mask)\u001B[0m\n\u001B[1;32m     48\u001B[0m attend_mask[attend_mask \u001B[38;5;241m!=\u001B[39m mol_length\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     49\u001B[0m attend_mask[attend_mask \u001B[38;5;241m==\u001B[39m mol_length\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 50\u001B[0m attend_mask \u001B[38;5;241m=\u001B[39m \u001B[43mattend_mask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFloatTensor\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     52\u001B[0m softmax_mask \u001B[38;5;241m=\u001B[39m atom_degree_list\u001B[38;5;241m.\u001B[39mclone()\n\u001B[1;32m     53\u001B[0m softmax_mask[softmax_mask \u001B[38;5;241m!=\u001B[39m mol_length\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e462a3d2d041829c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
